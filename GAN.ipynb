{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  as th\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import gzip as gz\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from BESTRq_classes.BESTRq import BestRqFramework, RandomProjectionQuantizer\n",
    "from compute_fft import compute_spectrogram, plot_spectrogram, mask_and_replace\n",
    "from models.CNN_BiLSTM_Attention import ParallelModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN implementation : comparison between spectrum and time serie \n",
    "https://towardsdatascience.com/synthetic-time-series-data-a-gan-approach-869a984f2239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [01:02<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/martinblot/Desktop/sleep-edf-prepared/5-cassette'  ## path towards the 5-cassette file\n",
    "fp = gz.open(data_path+'/SC4001E0.npz.gz','rb')\n",
    "data_test = np.load(fp,allow_pickle=True)\n",
    "fnames = glob.glob(os.path.join(data_path, \"*npz.gz\"))\n",
    "devpart = 10\n",
    "xtrain , xvalid = None , None\n",
    "ytrain , yvalid = None , None\n",
    "measurement=data_test['ch_label'][2]\n",
    "\n",
    "for fn in tqdm(fnames):\n",
    "    fp = gz.open(fn,'rb')\n",
    "    data = np.load(fp,allow_pickle=False) # for now, don't care about headers\n",
    "    x = data['x'][:,:,2] # EEG and EOG\n",
    "    y = data['y'] # Take the labels\n",
    "    idx = np.arange(x.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    devlim = x.shape[0]//devpart\n",
    "    devpart = 10\n",
    "    idx = np.arange(x.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    devlim = x.shape[0]//devpart\n",
    "    if xtrain is None:\n",
    "        xtrain = np.zeros((1,x.shape[1]))    ##np.zeros((1,x.shape[1],2)) if we include EOG  \n",
    "        xvalid = np.zeros((1,x.shape[1]))\n",
    "        ytrain , yvalid = np.zeros(1) , np.zeros(1)\n",
    "    xvalid = np.concatenate((xvalid,x[idx[:devlim]]), axis=0)\n",
    "    yvalid = np.concatenate((yvalid,y[idx[:devlim]]), axis=0)\n",
    "    xtrain = np.concatenate((xtrain,x[idx[devlim:]]), axis=0)\n",
    "    ytrain = np.concatenate((ytrain,y[idx[devlim:]]), axis=0)\n",
    "    del x,y\n",
    "\n",
    "xtrain , xvalid = xtrain[1:] , xvalid[1:]\n",
    "ytrain , yvalid = ytrain[1:] , yvalid[1:]\n",
    "xtrain, xvalid = th.FloatTensor(xtrain), th.FloatTensor(xvalid)\n",
    "ytrain, yvalid = th.IntTensor(ytrain), th.IntTensor(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175995, 600])\n"
     ]
    }
   ],
   "source": [
    "outf=\"/Users/martinblot/Desktop/sleep-edf-prepared/cassette-th-data-all.pck\"\n",
    "fp = open(outf,\"wb\")\n",
    "pickle.dump((xtrain , xvalid , ytrain , yvalid), fp)\n",
    "filepath = '/Users/martinblot/Desktop/sleep-edf-prepared/cassette-th-data-all.pck'\n",
    "xtrain,xvalid, ytrain, yvalid = np.load(filepath, allow_pickle = True)\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/0mw5kdt55tl7r35rc_2wqkph0000gn/T/ipykernel_32710/1359949598.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xtrain_tensor,ytrain_tensor=th.tensor(xtrain[:1000]),th.tensor(ytrain[:1000])\n",
      "/var/folders/4d/0mw5kdt55tl7r35rc_2wqkph0000gn/T/ipykernel_32710/1359949598.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xvalid_tensor,yvalid_tensor=th.tensor(xvalid[:1000]),th.tensor(yvalid[:1000])\n"
     ]
    }
   ],
   "source": [
    "batch=100\n",
    "xtrain_tensor,ytrain_tensor=th.tensor(xtrain[:1000]),th.tensor(ytrain[:1000])\n",
    "xvalid_tensor,yvalid_tensor=th.tensor(xvalid[:1000]),th.tensor(yvalid[:1000])\n",
    "\n",
    "dataset_t = TensorDataset(xtrain_tensor, ytrain_tensor)\n",
    "train_loader = DataLoader(dataset_t, batch_size= batch, shuffle=True)\n",
    "dataset_v = TensorDataset(xvalid_tensor, yvalid_tensor)\n",
    "valid_loader = DataLoader(dataset_v, batch_size= batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        # Couches du générateur\n",
    "        self.GRU=nn.GRU(1, hidden_dim,num_layers=n_layers,batch_first=True,bidirectional=True)\n",
    "        self.gen_block=nn.Linear(2*hidden_dim,1)\n",
    "        self.activation=nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x=self.GRU(x)\n",
    "        x = x[0]\n",
    "        x=self.gen_block(x)\n",
    "        x=self.activation(x)   ##classification binaire utile au GAN\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,hidden_dim,n_layers):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        # Couches du générateur\n",
    "        self.GRU=nn.GRU(1, hidden_dim,num_layers=n_layers,batch_first=True,bidirectional=True)\n",
    "        self.gen_block=nn.Linear(2*hidden_dim,1)\n",
    "        self.activation=nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x=self.GRU(x)\n",
    "        x = x[0]\n",
    "        x=self.gen_block(x)\n",
    "        x=self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, real_batch_labels, criterion):\n",
    "  loss = criterion(fake_output, real_batch_labels)\n",
    "  return loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, real_labels, fake_labels, criterion):\n",
    "    real_loss = criterion(real_output, real_labels)\n",
    "    fake_loss = criterion(fake_output, fake_labels)\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ydataai/ydata-synthetic/blob/dev/src/ydata_synthetic/synthesizers/timeseries/timegan/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN(discriminator, generator, batch, trainloader, hidden_dim, dis_loss, gen_loss, input_dim=600, lr = 0.0002, nepoch = 10):\n",
    "  real_batch_labels = th.ones(input_dim, 1)\n",
    "  fake_batch_labels = th.zeros(input_dim, 1)\n",
    "  optim_generator = optim.Adam(generator.parameters(), lr = lr)\n",
    "  optim_discriminator = optim.Adam(discriminator.parameters(), lr = lr)\n",
    "  g_losses, d_losses = [], []\n",
    "\n",
    "  for epoch in tqdm(range(nepoch)):\n",
    "      running_d_loss = 0\n",
    "      running_g_loss = 0\n",
    "\n",
    "      for inputs,_ in trainloader:\n",
    "          inputs=inputs.unsqueeze(-1)\n",
    "          z=th.randn((batch,input_dim,1))\n",
    "          # Train discriminator\n",
    "          outputs = discriminator(inputs)\n",
    "          h = generator(z)\n",
    "          fake_outputs = discriminator(h)\n",
    "          print(h.shape,fake_outputs.shape)\n",
    "\n",
    "          optim_discriminator.zero_grad()\n",
    "          d_loss = discriminator_loss(outputs,fake_outputs,real_batch_labels, fake_batch_labels, dis_loss)\n",
    "          d_loss.backward()\n",
    "          running_d_loss += d_loss.item()\n",
    "          optim_discriminator.step()\n",
    "\n",
    "          # Train generator\n",
    "          optim_generator.zero_grad()\n",
    "          fake_outputs = discriminator(h)\n",
    "          g_loss = generator_loss(fake_outputs,real_batch_labels,gen_loss)\n",
    "          g_loss.backward()\n",
    "          running_g_loss += g_loss.item()\n",
    "          optim_generator.step()\n",
    "\n",
    "      g_losses.append(running_g_loss / len(trainloader))\n",
    "      d_losses.append(running_d_loss / len(trainloader))\n",
    "\n",
    "      print(f\"Epoch {epoch + 1}/{nepoch} -> \"\n",
    "            f\"Generator Loss: {running_g_loss :.4f}, \"\n",
    "            f\"Discriminator Loss: {running_d_loss:.4f}\")\n",
    "\n",
    "\n",
    "  return g_losses, d_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 600, 1]) torch.Size([100, 600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([100, 600, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m bceloss\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     12\u001b[0m logloss\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[0;32m---> 14\u001b[0m g_losses, d_losses\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_GAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbceloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[152], line 22\u001b[0m, in \u001b[0;36mtrain_GAN\u001b[0;34m(discriminator, generator, batch, trainloader, hidden_dim, dis_loss, gen_loss, input_dim, lr, nepoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(h\u001b[38;5;241m.\u001b[39mshape,fake_outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m optim_discriminator\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfake_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreal_batch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_batch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdis_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m d_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m running_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[151], line 6\u001b[0m, in \u001b[0;36mdiscriminator_loss\u001b[0;34m(real_output, fake_output, real_labels, fake_labels, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscriminator_loss\u001b[39m(real_output, fake_output, real_labels, fake_labels, criterion):\n\u001b[0;32m----> 6\u001b[0m     real_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     fake_loss \u001b[38;5;241m=\u001b[39m criterion(fake_output, fake_labels)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m real_loss \u001b[38;5;241m+\u001b[39m fake_loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3121\u001b[0m     )\n\u001b[1;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([100, 600, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "learning_rate=0.0001\n",
    "hidden_dim = 128\n",
    "num_epochs = 10\n",
    "input_dim=600\n",
    "n_layers=5\n",
    "\n",
    "# Instanciation des modèles\n",
    "generator = Generator(hidden_dim,n_layers)\n",
    "discriminator = Discriminator(hidden_dim,n_layers)\n",
    "\n",
    "bceloss=nn.BCELoss()\n",
    "logloss=nn.NLLLoss()\n",
    "\n",
    "g_losses, d_losses=train_GAN(discriminator, generator, batch, train_loader, hidden_dim, bceloss, logloss, lr = 0.0002, nepoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gen_losses, label='Generator Loss')\n",
    "plt.plot(dis_losses, label='Discriminator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evolution de la perte pendant l\\'entraînement')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
