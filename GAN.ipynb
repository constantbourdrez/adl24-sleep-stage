{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  as th\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import gzip as gz\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from BESTRq_classes.BESTRq import BestRqFramework, RandomProjectionQuantizer\n",
    "from compute_fft import compute_spectrogram, plot_spectrogram, mask_and_replace\n",
    "from models.CNN_BiLSTM_Attention import ParallelModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN implementation : comparison between spectrum and time serie \n",
    "https://towardsdatascience.com/synthetic-time-series-data-a-gan-approach-869a984f2239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(model, n_layers, hidden_units, output_units):\n",
    "    for i in range(n_layers):\n",
    "        model.add_module(f'GRU_{i + 1}', nn.GRU(hidden_units, hidden_units, batch_first=True))\n",
    "        model.add_module('OUT', nn.Sequential(nn.Linear(hidden_units, output_units), nn.Sigmoid()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        model = nn.Sequential()\n",
    "        return make_net(model,\n",
    "                        n_layers=2,\n",
    "                        hidden_units=self.hidden_dim,\n",
    "                        output_units=self.hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,hidden_dim,data_dim):\n",
    "        super(Generator,self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.data_dim=data_dim\n",
    "        self.gen_block=nn.Sequential(\n",
    "    nn.Linear(hidden_dim, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, data_dim)\n",
    "    )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.gen_block(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,data_dim):\n",
    "        super(Generator,self).__init__()\n",
    "        self.data_dim=data_dim\n",
    "        self.dim_block=nn.Sequential(\n",
    "    nn.Linear(data_dim, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")   \n",
    "    def forward(self,x):\n",
    "        return self.dim_block(x)\n",
    "\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_seq):\n",
    "        super(Recovery, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_seq = n_seq\n",
    "        self.model = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.n_seq))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [01:02<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/martinblot/Desktop/sleep-edf-prepared/5-cassette'  ## path towards the 5-cassette file\n",
    "fp = gz.open(data_path+'/SC4001E0.npz.gz','rb')\n",
    "data_test = np.load(fp,allow_pickle=True)\n",
    "fnames = glob.glob(os.path.join(data_path, \"*npz.gz\"))\n",
    "devpart = 10\n",
    "xtrain , xvalid = None , None\n",
    "ytrain , yvalid = None , None\n",
    "measurement=data_test['ch_label'][2]\n",
    "\n",
    "for fn in tqdm(fnames):\n",
    "    fp = gz.open(fn,'rb')\n",
    "    data = np.load(fp,allow_pickle=False) # for now, don't care about headers\n",
    "    x = data['x'][:,:,2] # EEG and EOG\n",
    "    y = data['y'] # Take the labels\n",
    "    idx = np.arange(x.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    devlim = x.shape[0]//devpart\n",
    "    devpart = 10\n",
    "    idx = np.arange(x.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    devlim = x.shape[0]//devpart\n",
    "    if xtrain is None:\n",
    "        xtrain = np.zeros((1,x.shape[1]))    ##np.zeros((1,x.shape[1],2)) if we include EOG  \n",
    "        xvalid = np.zeros((1,x.shape[1]))\n",
    "        ytrain , yvalid = np.zeros(1) , np.zeros(1)\n",
    "    xvalid = np.concatenate((xvalid,x[idx[:devlim]]), axis=0)\n",
    "    yvalid = np.concatenate((yvalid,y[idx[:devlim]]), axis=0)\n",
    "    xtrain = np.concatenate((xtrain,x[idx[devlim:]]), axis=0)\n",
    "    ytrain = np.concatenate((ytrain,y[idx[devlim:]]), axis=0)\n",
    "    del x,y\n",
    "\n",
    "xtrain , xvalid = xtrain[1:] , xvalid[1:]\n",
    "ytrain , yvalid = ytrain[1:] , yvalid[1:]\n",
    "xtrain, xvalid = th.FloatTensor(xtrain), th.FloatTensor(xvalid)\n",
    "ytrain, yvalid = th.IntTensor(ytrain), th.IntTensor(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175995, 600])\n"
     ]
    }
   ],
   "source": [
    "outf=\"/Users/martinblot/Desktop/sleep-edf-prepared/cassette-th-data-all.pck\"\n",
    "fp = open(outf,\"wb\")\n",
    "pickle.dump((xtrain , xvalid , ytrain , yvalid), fp)\n",
    "filepath = '/Users/martinblot/Desktop/sleep-edf-prepared/cassette-th-data-all.pck'\n",
    "xtrain,xvalid, ytrain, yvalid = np.load(filepath, allow_pickle = True)\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/0mw5kdt55tl7r35rc_2wqkph0000gn/T/ipykernel_3563/1359949598.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xtrain_tensor,ytrain_tensor=th.tensor(xtrain[:1000]),th.tensor(ytrain[:1000])\n",
      "/var/folders/4d/0mw5kdt55tl7r35rc_2wqkph0000gn/T/ipykernel_3563/1359949598.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xvalid_tensor,yvalid_tensor=th.tensor(xvalid[:1000]),th.tensor(yvalid[:1000])\n"
     ]
    }
   ],
   "source": [
    "batch=100\n",
    "xtrain_tensor,ytrain_tensor=th.tensor(xtrain[:1000]),th.tensor(ytrain[:1000])\n",
    "xvalid_tensor,yvalid_tensor=th.tensor(xvalid[:1000]),th.tensor(yvalid[:1000])\n",
    "\n",
    "dataset_t = TensorDataset(xtrain_tensor, ytrain_tensor)\n",
    "train_loader = DataLoader(dataset_t, batch_size= batch, shuffle=True)\n",
    "dataset_v = TensorDataset(xvalid_tensor, yvalid_tensor)\n",
    "valid_loader = DataLoader(dataset_v, batch_size= batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du GAN\n",
    "generator = Generator(hidden_dim=128)\n",
    "discriminator = Discriminator(hidden_dim=128)\n",
    "supervisor = Supervisor(hidden_dim=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 600])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 64, got 600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Entraînement du discriminateur\u001b[39;00m\n\u001b[1;32m     35\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m real_decision \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m criterion(real_decision, th\u001b[38;5;241m.\u001b[39mones_like(real_decision))\n\u001b[1;32m     39\u001b[0m latent_vector \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn(batch, latent_dim)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[54], line 38\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1098\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1101\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:273\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:239\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 64, got 600"
     ]
    }
   ],
   "source": [
    "learning_rate=0.0002\n",
    "hidden_dim = 64\n",
    "latent_dim = 100\n",
    "num_epochs = 10\n",
    "\n",
    "# Instanciation des modèles\n",
    "generator = Generator(hidden_dim)\n",
    "discriminator = Discriminator(hidden_dim)\n",
    "recovery = Recovery(hidden_dim, n_seq=100)\n",
    "embedder = Embedder(hidden_dim)\n",
    "supervisor = Supervisor(hidden_dim)\n",
    "\n",
    "# Définition des optimiseurs\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "dis_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "supervisor_optimizer = optim.Adam(supervisor.parameters(), lr=learning_rate)\n",
    "\n",
    "# Définition des fonctions de perte\n",
    "criterion = nn.BCELoss()\n",
    "supervisor_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    " # Boucle d'apprentissage\n",
    "for epoch in range(num_epochs):\n",
    "    # Entraînement\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    supervisor.train()\n",
    "    train_loss = 0.0  # Initialisation de la perte d'entraînement\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    for real_data, labels in train_loader:\n",
    "        print(real_data.shape)\n",
    "        # Entraînement du discriminateur\n",
    "        discriminator.zero_grad()\n",
    "        real_decision = discriminator(real_data)\n",
    "        real_loss = criterion(real_decision, th.ones_like(real_decision))\n",
    "        \n",
    "        latent_vector = th.randn(batch, latent_dim)\n",
    "        fake_data = generator(latent_vector)\n",
    "        fake_decision = discriminator(fake_data.detach())\n",
    "        fake_loss = criterion(fake_decision, th.zeros_like(fake_decision))\n",
    "        \n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # Entraînement du superviseur\n",
    "        supervisor.zero_grad()\n",
    "        supervised_fake_data = supervisor(fake_data)\n",
    "        supervised_real_data = supervisor(real_data)\n",
    "        supervisor_loss = supervisor_criterion(supervised_fake_data, supervised_real_data.detach())\n",
    "        supervisor_loss.backward()\n",
    "        supervisor_optimizer.step()\n",
    "\n",
    "        # Entraînement du générateur\n",
    "        generator.zero_grad()\n",
    "        latent_vector = th.randn(batch, latent_dim)\n",
    "        fake_data = generator(latent_vector)\n",
    "        fake_decision = discriminator(fake_data)\n",
    "        g_loss = criterion(fake_decision, th.ones_like(fake_decision))\n",
    "        g_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Calcul de la perte d'entraînement et des prédictions correctes\n",
    "        train_loss += d_loss.item() + g_loss.item() + supervisor_loss.item()\n",
    "        _, predicted_labels = th.max(fake_decision, 1)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Normalisation de la perte d'entraînement par le nombre de lots\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Validation, calcul de la perte moyenne et de l'exactitude\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    supervisor.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with th.no_grad():\n",
    "        for real_data, labels in valid_loader:\n",
    "            real_decision = discriminator(real_data)\n",
    "            real_loss = criterion(real_decision, th.ones_like(real_decision))\n",
    "            \n",
    "            latent_vector = th.randn(batch, latent_dim)\n",
    "            fake_data = generator(latent_vector)\n",
    "            fake_decision = discriminator(fake_data.detach())\n",
    "            fake_loss = criterion(fake_decision, th.zeros_like(fake_decision))\n",
    "            \n",
    "            valid_loss += real_loss.item() + fake_loss.item()\n",
    "\n",
    "            # Calcul des prédictions correctes\n",
    "            _, predicted_labels = th.max(fake_decision, 1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Affichage de la progression, enregistrement des modèles, etc.\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}, Train Accuracy: {accuracy}, Valid Loss: {valid_loss}, Valid Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
